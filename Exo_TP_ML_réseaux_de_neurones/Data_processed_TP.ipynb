{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#importer les fichier csv\n",
        "output_data_processed = pd.read_csv(\"/content/Exercice2_sensor_raw.csv\")\n",
        "output_data_raw = pd.read_csv(\"/content/Exercice2_sensor_raw.csv\")\n",
        "\n",
        "output_data_processed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "ZD6lRyJHjP5v",
        "outputId": "7ef3c0e7-9bf5-485f-9ce5-f776f4bb4192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Target(Class)     GyroX     GyroY     GyroZ      AccX      AccY  \\\n",
              "0                 1 -0.923664  3.694656  0.824427  0.162598 -0.086670   \n",
              "1                 1 -0.908397  4.534351  0.832061  0.175781 -0.100586   \n",
              "2                 1  0.786260  3.969466  0.587786  0.322754 -0.140381   \n",
              "3                 1  0.335878  4.564885 -0.251908  0.480225 -0.226807   \n",
              "4                 1  3.351145  2.694656 -0.106870  0.426025 -0.253906   \n",
              "...             ...       ...       ...       ...       ...       ...   \n",
              "1109              4 -1.961832  2.358779  8.893130  0.472900 -0.431152   \n",
              "1110              4 -0.435115  5.396947  1.282443  0.459961 -0.227051   \n",
              "1111              4 -1.061069  4.534351  1.183206  0.419189 -0.192871   \n",
              "1112              4 -1.015267  5.259542  0.656489  0.308838 -0.090088   \n",
              "1113              4 -0.877863  4.488550  0.045802  0.098145 -0.015869   \n",
              "\n",
              "          AccZ  \n",
              "0    -0.969482  \n",
              "1    -1.013184  \n",
              "2    -0.911621  \n",
              "3    -0.936768  \n",
              "4    -0.950195  \n",
              "...        ...  \n",
              "1109 -0.873535  \n",
              "1110 -0.967041  \n",
              "1111 -1.002686  \n",
              "1112 -1.023193  \n",
              "1113 -1.001221  \n",
              "\n",
              "[1114 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88b302b0-a83c-4fa0-b0dc-14943afb7e61\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target(Class)</th>\n",
              "      <th>GyroX</th>\n",
              "      <th>GyroY</th>\n",
              "      <th>GyroZ</th>\n",
              "      <th>AccX</th>\n",
              "      <th>AccY</th>\n",
              "      <th>AccZ</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.923664</td>\n",
              "      <td>3.694656</td>\n",
              "      <td>0.824427</td>\n",
              "      <td>0.162598</td>\n",
              "      <td>-0.086670</td>\n",
              "      <td>-0.969482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.908397</td>\n",
              "      <td>4.534351</td>\n",
              "      <td>0.832061</td>\n",
              "      <td>0.175781</td>\n",
              "      <td>-0.100586</td>\n",
              "      <td>-1.013184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0.786260</td>\n",
              "      <td>3.969466</td>\n",
              "      <td>0.587786</td>\n",
              "      <td>0.322754</td>\n",
              "      <td>-0.140381</td>\n",
              "      <td>-0.911621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0.335878</td>\n",
              "      <td>4.564885</td>\n",
              "      <td>-0.251908</td>\n",
              "      <td>0.480225</td>\n",
              "      <td>-0.226807</td>\n",
              "      <td>-0.936768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>3.351145</td>\n",
              "      <td>2.694656</td>\n",
              "      <td>-0.106870</td>\n",
              "      <td>0.426025</td>\n",
              "      <td>-0.253906</td>\n",
              "      <td>-0.950195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1109</th>\n",
              "      <td>4</td>\n",
              "      <td>-1.961832</td>\n",
              "      <td>2.358779</td>\n",
              "      <td>8.893130</td>\n",
              "      <td>0.472900</td>\n",
              "      <td>-0.431152</td>\n",
              "      <td>-0.873535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1110</th>\n",
              "      <td>4</td>\n",
              "      <td>-0.435115</td>\n",
              "      <td>5.396947</td>\n",
              "      <td>1.282443</td>\n",
              "      <td>0.459961</td>\n",
              "      <td>-0.227051</td>\n",
              "      <td>-0.967041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1111</th>\n",
              "      <td>4</td>\n",
              "      <td>-1.061069</td>\n",
              "      <td>4.534351</td>\n",
              "      <td>1.183206</td>\n",
              "      <td>0.419189</td>\n",
              "      <td>-0.192871</td>\n",
              "      <td>-1.002686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1112</th>\n",
              "      <td>4</td>\n",
              "      <td>-1.015267</td>\n",
              "      <td>5.259542</td>\n",
              "      <td>0.656489</td>\n",
              "      <td>0.308838</td>\n",
              "      <td>-0.090088</td>\n",
              "      <td>-1.023193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1113</th>\n",
              "      <td>4</td>\n",
              "      <td>-0.877863</td>\n",
              "      <td>4.488550</td>\n",
              "      <td>0.045802</td>\n",
              "      <td>0.098145</td>\n",
              "      <td>-0.015869</td>\n",
              "      <td>-1.001221</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1114 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88b302b0-a83c-4fa0-b0dc-14943afb7e61')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-88b302b0-a83c-4fa0-b0dc-14943afb7e61 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-88b302b0-a83c-4fa0-b0dc-14943afb7e61');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ea0afc90-42c8-47ea-82f8-edd443446168\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ea0afc90-42c8-47ea-82f8-edd443446168')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ea0afc90-42c8-47ea-82f8-edd443446168 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "output_data_processed",
              "summary": "{\n  \"name\": \"output_data_processed\",\n  \"rows\": 1114,\n  \"fields\": [\n    {\n      \"column\": \"Target(Class)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GyroX\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.404296743524542,\n        \"min\": -14.94656488549618,\n        \"max\": 12.77862595419847,\n        \"num_unique_values\": 762,\n        \"samples\": [\n          -0.3740458015267176,\n          -1.251908396946565,\n          -3.1908396946564888\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GyroY\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.252790460321782,\n        \"min\": -10.35114503816794,\n        \"max\": 16.79389312977099,\n        \"num_unique_values\": 754,\n        \"samples\": [\n          10.587786259541993,\n          7.396946564885495,\n          5.213740458015267\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GyroZ\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.049384435388038,\n        \"min\": -50.25954198473283,\n        \"max\": 45.44274809160305,\n        \"num_unique_values\": 677,\n        \"samples\": [\n          0.3511450381679389,\n          -2.458015267175572,\n          0.8549618320610687\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AccX\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18315469426955572,\n        \"min\": -0.252685546875,\n        \"max\": 0.747802734375,\n        \"num_unique_values\": 919,\n        \"samples\": [\n          0.1796875,\n          0.353515625,\n          0.1171875\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AccY\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1904068197588887,\n        \"min\": -0.79345703125,\n        \"max\": 0.7685546875,\n        \"num_unique_values\": 847,\n        \"samples\": [\n          -0.198486328125,\n          -0.426513671875,\n          -0.17236328125\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AccZ\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09806108111693951,\n        \"min\": -1.367919921875,\n        \"max\": -0.456787109375,\n        \"num_unique_values\": 756,\n        \"samples\": [\n          -1.054443359375,\n          -1.05078125,\n          -0.96142578125\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Importer les fichiers CSV\n",
        "output_data_processed = pd.read_csv(\"/content/Exercice2_sensor_raw.csv\")\n",
        "\n",
        "# Extraire les classes uniques\n",
        "classes = output_data_processed[\"Target(Class)\"].unique()\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "\n",
        "\n",
        "\n",
        "# Calculer la matrice de corrélation pour chaque classe\n",
        "for classe in classes:\n",
        "    # Filtrer les données pour chaque classe\n",
        "    data_classe = output_data_processed[output_data_processed[\"Target(Class)\"] == classe]\n",
        "    variables = [\"GyroX\", \"GyroY\", \"GyroZ\", \"AccX\", \"AccY\", \"AccZ\"]\n",
        "    data_classe = data_classe[variables]\n",
        "\n",
        "    # Calculer la matrice de corrélation\n",
        "    correlation_matrix = output_data_processed.corr()\n",
        "    #print(f\"Correlation matrix for class {classe} (training data):\\n\", correlation_matrix)\n",
        "\n",
        "    # Vous pouvez décommenter ces lignes pour afficher la matrice de corrélation\n",
        "    # plt.figure(figsize=(8,6))\n",
        "    # sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    # plt.title(f\"Matrice de corrélation pour la classe {classe}\")\n",
        "    # plt.show()\n",
        "\n",
        "# Définir les variables indépendantes (X) et la variable dépendante (y)\n",
        "x = output_data_processed.iloc[:, 1:]\n",
        "y = output_data_processed[\"Target(Class)\"].values - 1\n",
        "\n",
        "# Séparer les données en ensembles d'apprentissage et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normaliser les données\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#changer en tensor.torch\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "#creation des dataloader pour pytorch\n",
        "train_dataset = TensorDataset(X_train_tensor,y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor,y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=10,shuffle= True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=10,shuffle= True)\n",
        "print(X_train)\n",
        "print(y_train)\n",
        "print(X_test_tensor)\n",
        "print(y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZKkmsUzqB0u",
        "outputId": "97b5e3f9-c842-4a6b-c7fa-888af2292cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.38818058 -1.75796387 -3.26667484 -2.03435387 -2.8285342  -1.72634825]\n",
            " [-0.84950093  0.94482839  0.24825092  0.64886866 -0.2518858  -0.65295584]\n",
            " [ 0.42114234  0.02192372 -0.04590334  1.41684394 -0.80339228  0.68320701]\n",
            " ...\n",
            " [-0.35023933  0.33269774 -0.05398792 -1.17792071  0.78282131 -0.57114995]\n",
            " [ 0.11303946  0.60815653 -0.02102772  2.10306269 -1.32707046  0.30640413]\n",
            " [-0.06687463 -0.59491563 -0.07699788 -0.07219713  0.2351096   0.88896122]]\n",
            "[1 1 3 3 2 2 2 2 1 1 0 0 3 1 3 0 1 2 3 0 1 1 1 1 1 1 3 3 1 1 2 2 2 3 2 1 0\n",
            " 1 0 0 0 3 0 0 1 2 0 2 1 1 1 1 2 1 3 2 1 2 1 2 2 1 0 2 0 3 1 3 1 1 3 1 0 2\n",
            " 1 0 1 2 1 1 1 1 1 1 2 2 2 0 2 2 3 1 2 3 0 0 0 1 1 3 1 2 2 2 2 3 2 0 1 3 2\n",
            " 1 2 1 1 0 0 2 3 1 1 2 3 0 2 2 0 0 1 0 2 1 0 1 3 1 0 0 3 0 3 2 2 2 2 1 2 1\n",
            " 3 0 1 2 0 0 2 2 1 1 2 0 1 2 1 2 0 3 0 3 1 2 0 2 3 0 3 1 3 0 1 0 2 1 0 1 1\n",
            " 2 2 2 0 2 0 2 0 3 2 0 3 2 1 3 0 3 3 0 2 1 1 3 0 3 0 2 1 2 0 1 2 2 3 1 2 2\n",
            " 3 2 3 3 2 3 2 3 1 2 3 2 0 0 3 2 0 3 0 0 1 3 2 2 1 0 2 0 2 1 2 1 1 2 2 3 3\n",
            " 3 3 0 1 1 1 1 3 1 2 2 1 2 3 0 0 1 0 2 2 2 3 0 3 0 1 3 1 2 0 0 0 1 2 1 2 1\n",
            " 2 2 1 1 2 0 0 1 0 3 0 2 0 1 2 2 1 0 2 0 0 3 0 2 2 0 0 0 2 3 2 1 1 0 3 3 2\n",
            " 2 1 2 1 1 1 3 3 1 1 0 0 2 2 0 1 3 1 2 1 2 3 2 2 2 2 1 1 2 2 1 1 0 3 1 2 0\n",
            " 1 0 3 1 3 2 0 1 3 3 0 0 1 2 3 1 2 3 3 2 2 1 1 0 3 1 3 3 0 0 0 1 0 1 2 3 2\n",
            " 3 1 1 2 3 0 2 0 2 0 2 1 3 1 0 0 2 2 1 0 1 0 1 3 2 0 3 3 1 2 2 2 2 0 0 1 2\n",
            " 0 2 2 2 3 2 0 2 1 1 2 0 1 0 0 3 3 3 3 1 3 1 2 2 3 1 1 2 1 0 0 3 3 2 2 0 1\n",
            " 3 0 1 2 1 2 1 0 2 3 3 3 1 1 3 2 0 0 0 3 2 3 1 0 2 2 1 2 2 2 2 3 2 0 0 1 1\n",
            " 2 3 0 0 3 2 2 0 1 3 2 2 2 3 2 1 1 0 1 0 3 0 0 1 0 0 3 1 3 2 1 1 2 2 0 0 3\n",
            " 3 1 3 1 0 0 2 2 3 1 1 3 2 0 2 2 0 3 3 1 3 3 2 2 0 2 3 0 2 2 0 2 2 3 0 0 2\n",
            " 0 1 1 3 2 3 2 0 0 0 1 3 0 3 2 3 3 0 3 0 1 2 1 1 3 2 2 2 2 2 1 2 1 1 0 3 2\n",
            " 2 3 2 3 1 2 2 0 0 2 1 3 3 2 2 1 1 1 0 0 3 2 0 2 3 0 2 2 1 1 3 1 0 0 0 1 2\n",
            " 0 1 3 3 3 3 0 0 2 1 3 3 2 2 3 0 2 0 3 3 0 2 3 3 1 3 2 1 1 3 1 1 2 0 2 0 1\n",
            " 2 2 1 2 2 3 0 2 3 2 1 2 1 3 2 1 1 2 2 2 1 1 1 0 3 3 1 1 1 2 3 3 2 1 2 2 0\n",
            " 3 2 3 3 2 2 2 2 0 0 2 2 0 1 0 2 0 2 2 3 3 1 2 3 0 2 0 2 2 2 2 1 3 2 2 3 0\n",
            " 1 2 2 1 2 3 0 0 2 2 1 3 2 2 3 2 3 2 3 2 0 2 2 2 1 1 3 2 0 0 0 3 0 1 1 1 1\n",
            " 2 2 1 0 0 1 1 0 2 0 2 2 1 2 1 3 1 2 1 3 0 0 2 1 2 2 3 3 0 0 2 1 3 0 3 2 2\n",
            " 0 0 1 2 1 0 2 0 2 2 2 3 2 3 0 3 2 1 3 1 2 2 1 0 1 1 3 1 2 1 2 0 2 0 1 1 0\n",
            " 3 3 2]\n",
            "tensor([[-5.7879e-02,  9.0200e-02, -3.0978e-02,  9.2630e-01, -3.0881e-01,\n",
            "          6.1380e-01],\n",
            "        [-1.4702e+00, -6.5187e-02, -3.9063e-02,  1.2104e+00, -4.1253e-01,\n",
            "         -2.0819e-02],\n",
            "        [-8.7115e-02,  1.3729e-01, -1.2943e-02,  9.3703e-01, -5.4282e-01,\n",
            "         -1.5220e-01],\n",
            "        ...,\n",
            "        [-1.0789e+00,  2.6913e-01, -6.3316e-02,  3.8350e-01, -6.3412e-02,\n",
            "          7.3031e-01],\n",
            "        [-1.0960e-01,  3.0889e-03, -4.6525e-02, -3.9118e-01, -4.5703e-02,\n",
            "         -6.0483e-02],\n",
            "        [ 1.1838e-02,  8.5491e-02, -4.2172e-02, -3.0406e-01,  1.0988e-03,\n",
            "         -4.3130e-02]])\n",
            "tensor([2, 0, 3, 2, 1, 2, 3, 1, 0, 3, 2, 0, 3, 2, 2, 1, 1, 0, 1, 2, 0, 0, 2, 0,\n",
            "        0, 1, 3, 2, 1, 1, 3, 2, 2, 2, 1, 0, 0, 2, 2, 1, 1, 1, 3, 3, 2, 2, 2, 3,\n",
            "        0, 1, 0, 1, 1, 3, 0, 2, 3, 0, 2, 1, 1, 3, 3, 3, 1, 1, 1, 3, 0, 1, 2, 2,\n",
            "        2, 2, 0, 2, 2, 2, 3, 1, 1, 3, 3, 2, 2, 2, 1, 0, 1, 1, 3, 3, 2, 0, 2, 2,\n",
            "        1, 0, 3, 1, 0, 0, 1, 2, 2, 1, 2, 2, 0, 0, 0, 0, 1, 2, 2, 1, 0, 1, 0, 0,\n",
            "        2, 3, 2, 3, 3, 1, 2, 1, 0, 1, 2, 3, 2, 3, 0, 2, 3, 3, 1, 1, 1, 1, 1, 0,\n",
            "        2, 2, 1, 0, 0, 2, 3, 0, 3, 2, 2, 2, 0, 0, 1, 2, 0, 0, 1, 3, 1, 1, 2, 0,\n",
            "        0, 3, 0, 0, 2, 1, 1, 2, 2, 2, 0, 2, 3, 3, 2, 3, 3, 3, 1, 0, 1, 0, 0, 1,\n",
            "        2, 1, 0, 0, 2, 1, 3, 1, 2, 2, 1, 3, 1, 3, 3, 0, 2, 0, 3, 0, 1, 2, 2, 1,\n",
            "        3, 2, 0, 2, 1, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Creation du modele\n",
        "## TODO: Define the NN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6, 512)\n",
        "        self.fc2 = nn.Linear(512, 224)\n",
        "        self.fc3 = nn.Linear(224, 112)\n",
        "        self.fc4 = nn.Linear(112, 60)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # flatten image input\n",
        "        x = x.view(-1, 6)\n",
        "        # Première couche Relu\n",
        "        x = F.relu(self.fc1(x))\n",
        "        #m = nn.Dropout(p=0.2)\n",
        "        #output = m(x)\n",
        "        # Deuxième couche ReLu\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # add hidden layer, with relu activation function\n",
        "        x = F.relu(self.fc3(x))\n",
        "        # Quatrième couche leaky ReLu\n",
        "        x = F.relu(self.fc4(x))\n",
        "\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "# initialize the NN\n",
        "model = Net()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVe3mHJ0quUI",
        "outputId": "20dee3bd-2c7e-4674-b7da-727916a59fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=6, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=224, bias=True)\n",
            "  (fc3): Linear(in_features=224, out_features=112, bias=True)\n",
            "  (fc4): Linear(in_features=112, out_features=60, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Specify loss and optimization functions\n",
        "import torch.optim as optim\n",
        "\n",
        "# specify loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "QYUJNpHp7klZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 300  # suggest training between 20-50 epochs\n",
        "\n",
        "model.train() # prep model for training\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    for data, target in train_loader:\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        "    # print training statistics\n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch+1,\n",
        "        train_loss\n",
        "        ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JX2NNF17skB",
        "outputId": "3ef68d91-9bc8-46ce-bf48-7abdf083eb42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 1.576405\n",
            "Epoch: 2 \tTraining Loss: 1.006148\n",
            "Epoch: 3 \tTraining Loss: 0.893469\n",
            "Epoch: 4 \tTraining Loss: 0.847361\n",
            "Epoch: 5 \tTraining Loss: 0.830747\n",
            "Epoch: 6 \tTraining Loss: 0.792741\n",
            "Epoch: 7 \tTraining Loss: 0.762932\n",
            "Epoch: 8 \tTraining Loss: 0.764454\n",
            "Epoch: 9 \tTraining Loss: 0.739159\n",
            "Epoch: 10 \tTraining Loss: 0.698409\n",
            "Epoch: 11 \tTraining Loss: 0.693622\n",
            "Epoch: 12 \tTraining Loss: 0.669615\n",
            "Epoch: 13 \tTraining Loss: 0.688860\n",
            "Epoch: 14 \tTraining Loss: 0.644919\n",
            "Epoch: 15 \tTraining Loss: 0.639631\n",
            "Epoch: 16 \tTraining Loss: 0.645205\n",
            "Epoch: 17 \tTraining Loss: 0.643818\n",
            "Epoch: 18 \tTraining Loss: 0.592903\n",
            "Epoch: 19 \tTraining Loss: 0.584867\n",
            "Epoch: 20 \tTraining Loss: 0.556118\n",
            "Epoch: 21 \tTraining Loss: 0.548495\n",
            "Epoch: 22 \tTraining Loss: 0.545332\n",
            "Epoch: 23 \tTraining Loss: 0.555899\n",
            "Epoch: 24 \tTraining Loss: 0.527294\n",
            "Epoch: 25 \tTraining Loss: 0.522176\n",
            "Epoch: 26 \tTraining Loss: 0.527602\n",
            "Epoch: 27 \tTraining Loss: 0.493066\n",
            "Epoch: 28 \tTraining Loss: 0.482935\n",
            "Epoch: 29 \tTraining Loss: 0.495082\n",
            "Epoch: 30 \tTraining Loss: 0.473160\n",
            "Epoch: 31 \tTraining Loss: 0.454827\n",
            "Epoch: 32 \tTraining Loss: 0.477587\n",
            "Epoch: 33 \tTraining Loss: 0.435257\n",
            "Epoch: 34 \tTraining Loss: 0.442742\n",
            "Epoch: 35 \tTraining Loss: 0.437591\n",
            "Epoch: 36 \tTraining Loss: 0.432216\n",
            "Epoch: 37 \tTraining Loss: 0.460830\n",
            "Epoch: 38 \tTraining Loss: 0.388842\n",
            "Epoch: 39 \tTraining Loss: 0.408543\n",
            "Epoch: 40 \tTraining Loss: 0.403162\n",
            "Epoch: 41 \tTraining Loss: 0.375620\n",
            "Epoch: 42 \tTraining Loss: 0.354764\n",
            "Epoch: 43 \tTraining Loss: 0.347405\n",
            "Epoch: 44 \tTraining Loss: 0.346746\n",
            "Epoch: 45 \tTraining Loss: 0.327806\n",
            "Epoch: 46 \tTraining Loss: 0.323948\n",
            "Epoch: 47 \tTraining Loss: 0.316133\n",
            "Epoch: 48 \tTraining Loss: 0.297147\n",
            "Epoch: 49 \tTraining Loss: 0.329413\n",
            "Epoch: 50 \tTraining Loss: 0.296352\n",
            "Epoch: 51 \tTraining Loss: 0.256522\n",
            "Epoch: 52 \tTraining Loss: 0.262455\n",
            "Epoch: 53 \tTraining Loss: 0.321516\n",
            "Epoch: 54 \tTraining Loss: 0.294151\n",
            "Epoch: 55 \tTraining Loss: 0.284907\n",
            "Epoch: 56 \tTraining Loss: 0.280845\n",
            "Epoch: 57 \tTraining Loss: 0.233977\n",
            "Epoch: 58 \tTraining Loss: 0.228268\n",
            "Epoch: 59 \tTraining Loss: 0.216511\n",
            "Epoch: 60 \tTraining Loss: 0.252912\n",
            "Epoch: 61 \tTraining Loss: 0.245425\n",
            "Epoch: 62 \tTraining Loss: 0.263172\n",
            "Epoch: 63 \tTraining Loss: 0.242619\n",
            "Epoch: 64 \tTraining Loss: 0.178023\n",
            "Epoch: 65 \tTraining Loss: 0.175937\n",
            "Epoch: 66 \tTraining Loss: 0.190046\n",
            "Epoch: 67 \tTraining Loss: 0.160973\n",
            "Epoch: 68 \tTraining Loss: 0.177362\n",
            "Epoch: 69 \tTraining Loss: 0.161631\n",
            "Epoch: 70 \tTraining Loss: 0.153876\n",
            "Epoch: 71 \tTraining Loss: 0.155703\n",
            "Epoch: 72 \tTraining Loss: 0.165019\n",
            "Epoch: 73 \tTraining Loss: 0.154522\n",
            "Epoch: 74 \tTraining Loss: 0.185039\n",
            "Epoch: 75 \tTraining Loss: 0.200318\n",
            "Epoch: 76 \tTraining Loss: 0.184778\n",
            "Epoch: 77 \tTraining Loss: 0.445899\n",
            "Epoch: 78 \tTraining Loss: 0.253591\n",
            "Epoch: 79 \tTraining Loss: 0.167863\n",
            "Epoch: 80 \tTraining Loss: 0.132037\n",
            "Epoch: 81 \tTraining Loss: 0.133729\n",
            "Epoch: 82 \tTraining Loss: 0.106011\n",
            "Epoch: 83 \tTraining Loss: 0.096321\n",
            "Epoch: 84 \tTraining Loss: 0.111969\n",
            "Epoch: 85 \tTraining Loss: 0.133821\n",
            "Epoch: 86 \tTraining Loss: 0.090843\n",
            "Epoch: 87 \tTraining Loss: 0.092505\n",
            "Epoch: 88 \tTraining Loss: 0.108403\n",
            "Epoch: 89 \tTraining Loss: 0.133695\n",
            "Epoch: 90 \tTraining Loss: 0.112684\n",
            "Epoch: 91 \tTraining Loss: 0.174927\n",
            "Epoch: 92 \tTraining Loss: 0.091675\n",
            "Epoch: 93 \tTraining Loss: 0.085917\n",
            "Epoch: 94 \tTraining Loss: 0.078230\n",
            "Epoch: 95 \tTraining Loss: 0.060424\n",
            "Epoch: 96 \tTraining Loss: 0.060123\n",
            "Epoch: 97 \tTraining Loss: 0.060737\n",
            "Epoch: 98 \tTraining Loss: 0.086904\n",
            "Epoch: 99 \tTraining Loss: 0.174346\n",
            "Epoch: 100 \tTraining Loss: 0.161807\n",
            "Epoch: 101 \tTraining Loss: 0.114764\n",
            "Epoch: 102 \tTraining Loss: 0.106040\n",
            "Epoch: 103 \tTraining Loss: 0.131711\n",
            "Epoch: 104 \tTraining Loss: 0.103311\n",
            "Epoch: 105 \tTraining Loss: 0.115868\n",
            "Epoch: 106 \tTraining Loss: 0.082311\n",
            "Epoch: 107 \tTraining Loss: 0.099328\n",
            "Epoch: 108 \tTraining Loss: 0.157904\n",
            "Epoch: 109 \tTraining Loss: 0.098142\n",
            "Epoch: 110 \tTraining Loss: 0.060501\n",
            "Epoch: 111 \tTraining Loss: 0.056222\n",
            "Epoch: 112 \tTraining Loss: 0.045252\n",
            "Epoch: 113 \tTraining Loss: 0.047318\n",
            "Epoch: 114 \tTraining Loss: 0.043417\n",
            "Epoch: 115 \tTraining Loss: 0.049161\n",
            "Epoch: 116 \tTraining Loss: 0.049719\n",
            "Epoch: 117 \tTraining Loss: 0.302305\n",
            "Epoch: 118 \tTraining Loss: 0.108489\n",
            "Epoch: 119 \tTraining Loss: 0.135992\n",
            "Epoch: 120 \tTraining Loss: 0.094646\n",
            "Epoch: 121 \tTraining Loss: 0.081212\n",
            "Epoch: 122 \tTraining Loss: 0.061370\n",
            "Epoch: 123 \tTraining Loss: 0.044181\n",
            "Epoch: 124 \tTraining Loss: 0.040230\n",
            "Epoch: 125 \tTraining Loss: 0.041436\n",
            "Epoch: 126 \tTraining Loss: 0.046572\n",
            "Epoch: 127 \tTraining Loss: 0.043120\n",
            "Epoch: 128 \tTraining Loss: 0.250089\n",
            "Epoch: 129 \tTraining Loss: 0.185145\n",
            "Epoch: 130 \tTraining Loss: 0.140713\n",
            "Epoch: 131 \tTraining Loss: 0.067868\n",
            "Epoch: 132 \tTraining Loss: 0.046294\n",
            "Epoch: 133 \tTraining Loss: 0.052902\n",
            "Epoch: 134 \tTraining Loss: 0.037613\n",
            "Epoch: 135 \tTraining Loss: 0.039191\n",
            "Epoch: 136 \tTraining Loss: 0.038697\n",
            "Epoch: 137 \tTraining Loss: 0.042766\n",
            "Epoch: 138 \tTraining Loss: 0.033462\n",
            "Epoch: 139 \tTraining Loss: 0.029145\n",
            "Epoch: 140 \tTraining Loss: 0.033666\n",
            "Epoch: 141 \tTraining Loss: 0.029935\n",
            "Epoch: 142 \tTraining Loss: 0.111310\n",
            "Epoch: 143 \tTraining Loss: 0.054573\n",
            "Epoch: 144 \tTraining Loss: 0.137701\n",
            "Epoch: 145 \tTraining Loss: 0.106909\n",
            "Epoch: 146 \tTraining Loss: 0.053827\n",
            "Epoch: 147 \tTraining Loss: 0.051811\n",
            "Epoch: 148 \tTraining Loss: 0.090323\n",
            "Epoch: 149 \tTraining Loss: 0.054443\n",
            "Epoch: 150 \tTraining Loss: 0.034131\n",
            "Epoch: 151 \tTraining Loss: 0.037908\n",
            "Epoch: 152 \tTraining Loss: 0.030358\n",
            "Epoch: 153 \tTraining Loss: 0.028937\n",
            "Epoch: 154 \tTraining Loss: 0.023515\n",
            "Epoch: 155 \tTraining Loss: 0.032873\n",
            "Epoch: 156 \tTraining Loss: 0.026803\n",
            "Epoch: 157 \tTraining Loss: 0.025477\n",
            "Epoch: 158 \tTraining Loss: 0.027800\n",
            "Epoch: 159 \tTraining Loss: 0.021443\n",
            "Epoch: 160 \tTraining Loss: 0.024873\n",
            "Epoch: 161 \tTraining Loss: 0.019402\n",
            "Epoch: 162 \tTraining Loss: 0.020342\n",
            "Epoch: 163 \tTraining Loss: 0.021773\n",
            "Epoch: 164 \tTraining Loss: 0.023638\n",
            "Epoch: 165 \tTraining Loss: 0.018709\n",
            "Epoch: 166 \tTraining Loss: 0.020231\n",
            "Epoch: 167 \tTraining Loss: 0.021619\n",
            "Epoch: 168 \tTraining Loss: 0.021052\n",
            "Epoch: 169 \tTraining Loss: 0.070072\n",
            "Epoch: 170 \tTraining Loss: 0.576843\n",
            "Epoch: 171 \tTraining Loss: 0.214300\n",
            "Epoch: 172 \tTraining Loss: 0.092803\n",
            "Epoch: 173 \tTraining Loss: 0.035300\n",
            "Epoch: 174 \tTraining Loss: 0.025875\n",
            "Epoch: 175 \tTraining Loss: 0.023130\n",
            "Epoch: 176 \tTraining Loss: 0.020306\n",
            "Epoch: 177 \tTraining Loss: 0.021548\n",
            "Epoch: 178 \tTraining Loss: 0.017304\n",
            "Epoch: 179 \tTraining Loss: 0.027091\n",
            "Epoch: 180 \tTraining Loss: 0.022493\n",
            "Epoch: 181 \tTraining Loss: 0.021933\n",
            "Epoch: 182 \tTraining Loss: 0.016554\n",
            "Epoch: 183 \tTraining Loss: 0.022444\n",
            "Epoch: 184 \tTraining Loss: 0.022825\n",
            "Epoch: 185 \tTraining Loss: 0.020845\n",
            "Epoch: 186 \tTraining Loss: 0.191129\n",
            "Epoch: 187 \tTraining Loss: 0.205870\n",
            "Epoch: 188 \tTraining Loss: 0.106429\n",
            "Epoch: 189 \tTraining Loss: 0.048016\n",
            "Epoch: 190 \tTraining Loss: 0.028086\n",
            "Epoch: 191 \tTraining Loss: 0.020109\n",
            "Epoch: 192 \tTraining Loss: 0.020328\n",
            "Epoch: 193 \tTraining Loss: 0.022214\n",
            "Epoch: 194 \tTraining Loss: 0.019176\n",
            "Epoch: 195 \tTraining Loss: 0.015824\n",
            "Epoch: 196 \tTraining Loss: 0.015478\n",
            "Epoch: 197 \tTraining Loss: 0.014384\n",
            "Epoch: 198 \tTraining Loss: 0.020867\n",
            "Epoch: 199 \tTraining Loss: 0.015360\n",
            "Epoch: 200 \tTraining Loss: 0.014667\n",
            "Epoch: 201 \tTraining Loss: 0.015576\n",
            "Epoch: 202 \tTraining Loss: 0.014121\n",
            "Epoch: 203 \tTraining Loss: 0.015568\n",
            "Epoch: 204 \tTraining Loss: 0.012947\n",
            "Epoch: 205 \tTraining Loss: 0.016179\n",
            "Epoch: 206 \tTraining Loss: 0.032223\n",
            "Epoch: 207 \tTraining Loss: 0.016030\n",
            "Epoch: 208 \tTraining Loss: 0.018782\n",
            "Epoch: 209 \tTraining Loss: 0.013233\n",
            "Epoch: 210 \tTraining Loss: 0.026345\n",
            "Epoch: 211 \tTraining Loss: 0.018024\n",
            "Epoch: 212 \tTraining Loss: 0.015824\n",
            "Epoch: 213 \tTraining Loss: 0.018697\n",
            "Epoch: 214 \tTraining Loss: 0.018558\n",
            "Epoch: 215 \tTraining Loss: 0.219348\n",
            "Epoch: 216 \tTraining Loss: 0.427550\n",
            "Epoch: 217 \tTraining Loss: 0.099345\n",
            "Epoch: 218 \tTraining Loss: 0.033184\n",
            "Epoch: 219 \tTraining Loss: 0.018597\n",
            "Epoch: 220 \tTraining Loss: 0.015809\n",
            "Epoch: 221 \tTraining Loss: 0.011499\n",
            "Epoch: 222 \tTraining Loss: 0.011281\n",
            "Epoch: 223 \tTraining Loss: 0.014837\n",
            "Epoch: 224 \tTraining Loss: 0.012105\n",
            "Epoch: 225 \tTraining Loss: 0.013134\n",
            "Epoch: 226 \tTraining Loss: 0.027004\n",
            "Epoch: 227 \tTraining Loss: 0.021462\n",
            "Epoch: 228 \tTraining Loss: 0.014244\n",
            "Epoch: 229 \tTraining Loss: 0.007872\n",
            "Epoch: 230 \tTraining Loss: 0.014299\n",
            "Epoch: 231 \tTraining Loss: 0.010548\n",
            "Epoch: 232 \tTraining Loss: 0.007099\n",
            "Epoch: 233 \tTraining Loss: 0.012751\n",
            "Epoch: 234 \tTraining Loss: 0.009726\n",
            "Epoch: 235 \tTraining Loss: 0.008901\n",
            "Epoch: 236 \tTraining Loss: 0.009868\n",
            "Epoch: 237 \tTraining Loss: 0.009241\n",
            "Epoch: 238 \tTraining Loss: 0.008486\n",
            "Epoch: 239 \tTraining Loss: 0.007584\n",
            "Epoch: 240 \tTraining Loss: 0.015186\n",
            "Epoch: 241 \tTraining Loss: 0.010887\n",
            "Epoch: 242 \tTraining Loss: 0.008506\n",
            "Epoch: 243 \tTraining Loss: 0.006878\n",
            "Epoch: 244 \tTraining Loss: 0.005489\n",
            "Epoch: 245 \tTraining Loss: 0.004295\n",
            "Epoch: 246 \tTraining Loss: 0.005051\n",
            "Epoch: 247 \tTraining Loss: 0.012825\n",
            "Epoch: 248 \tTraining Loss: 0.050938\n",
            "Epoch: 249 \tTraining Loss: 0.458562\n",
            "Epoch: 250 \tTraining Loss: 0.254564\n",
            "Epoch: 251 \tTraining Loss: 0.084256\n",
            "Epoch: 252 \tTraining Loss: 0.027837\n",
            "Epoch: 253 \tTraining Loss: 0.011131\n",
            "Epoch: 254 \tTraining Loss: 0.007926\n",
            "Epoch: 255 \tTraining Loss: 0.008572\n",
            "Epoch: 256 \tTraining Loss: 0.007614\n",
            "Epoch: 257 \tTraining Loss: 0.005445\n",
            "Epoch: 258 \tTraining Loss: 0.006418\n",
            "Epoch: 259 \tTraining Loss: 0.007758\n",
            "Epoch: 260 \tTraining Loss: 0.007338\n",
            "Epoch: 261 \tTraining Loss: 0.011455\n",
            "Epoch: 262 \tTraining Loss: 0.011166\n",
            "Epoch: 263 \tTraining Loss: 0.005529\n",
            "Epoch: 264 \tTraining Loss: 0.004503\n",
            "Epoch: 265 \tTraining Loss: 0.005116\n",
            "Epoch: 266 \tTraining Loss: 0.008237\n",
            "Epoch: 267 \tTraining Loss: 0.022773\n",
            "Epoch: 268 \tTraining Loss: 0.016754\n",
            "Epoch: 269 \tTraining Loss: 0.016298\n",
            "Epoch: 270 \tTraining Loss: 0.005079\n",
            "Epoch: 271 \tTraining Loss: 0.004887\n",
            "Epoch: 272 \tTraining Loss: 0.003771\n",
            "Epoch: 273 \tTraining Loss: 0.003881\n",
            "Epoch: 274 \tTraining Loss: 0.004951\n",
            "Epoch: 275 \tTraining Loss: 0.007077\n",
            "Epoch: 276 \tTraining Loss: 0.004188\n",
            "Epoch: 277 \tTraining Loss: 0.002784\n",
            "Epoch: 278 \tTraining Loss: 0.003756\n",
            "Epoch: 279 \tTraining Loss: 0.003575\n",
            "Epoch: 280 \tTraining Loss: 0.011590\n",
            "Epoch: 281 \tTraining Loss: 0.332831\n",
            "Epoch: 282 \tTraining Loss: 0.148505\n",
            "Epoch: 283 \tTraining Loss: 0.127647\n",
            "Epoch: 284 \tTraining Loss: 0.071819\n",
            "Epoch: 285 \tTraining Loss: 0.063405\n",
            "Epoch: 286 \tTraining Loss: 0.021558\n",
            "Epoch: 287 \tTraining Loss: 0.005395\n",
            "Epoch: 288 \tTraining Loss: 0.005779\n",
            "Epoch: 289 \tTraining Loss: 0.004686\n",
            "Epoch: 290 \tTraining Loss: 0.005440\n",
            "Epoch: 291 \tTraining Loss: 0.004480\n",
            "Epoch: 292 \tTraining Loss: 0.004044\n",
            "Epoch: 293 \tTraining Loss: 0.003353\n",
            "Epoch: 294 \tTraining Loss: 0.002934\n",
            "Epoch: 295 \tTraining Loss: 0.002637\n",
            "Epoch: 296 \tTraining Loss: 0.011218\n",
            "Epoch: 297 \tTraining Loss: 0.005617\n",
            "Epoch: 298 \tTraining Loss: 0.008905\n",
            "Epoch: 299 \tTraining Loss: 0.006630\n",
            "Epoch: 300 \tTraining Loss: 0.002853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize lists to monitor test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval() # prep model for *evaluation*\n",
        "\n",
        "for data, target in test_loader:\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss\n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "    # calculate test accuracy for each object class 32 = batchsize\n",
        "    for i in range(len(target)):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(4):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\n",
        "            class_correct[i], class_total[i]))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2-KnPdq73Sf",
        "outputId": "6fcf54d2-567a-467d-d1fc-583697432a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 5.054336\n",
            "\n",
            "Test Accuracy of     0: 38% (20/52)\n",
            "Test Accuracy of     1: 62% (37/59)\n",
            "Test Accuracy of     2: 50% (35/69)\n",
            "Test Accuracy of     3: 69% (30/43)\n",
            "\n",
            "Test Accuracy (Overall): 54% (122/223)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Mettre le modèle en mode évaluation\n",
        "model.eval()\n",
        "\n",
        "# Listes pour stocker les vraies étiquettes et les prédictions\n",
        "all_targets = []\n",
        "all_predictions = []\n",
        "\n",
        "# Désactiver le calcul des gradients pour plus de vitesse\n",
        "for data, target in test_loader:\n",
        "      # Faire des prédictions\n",
        "      output = model(data)\n",
        "      _, predicted = torch.max(output, 1)\n",
        "\n",
        "      # Stocker les vraies étiquettes et les prédictions\n",
        "      all_targets.extend(target.numpy())\n",
        "      all_predictions.extend(predicted.numpy())\n",
        "\n",
        "# Convertir les listes en tableaux numpy\n",
        "all_targets = np.array(all_targets)\n",
        "all_predictions = np.array(all_predictions)\n",
        "\n",
        "# Créer la matrice de confusion\n",
        "conf_matrix = confusion_matrix(all_targets, all_predictions)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN2pWLvn769Y",
        "outputId": "33a25bf9-faec-4b25-9ad1-6ba0759ee115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[23 11 15  3]\n",
            " [ 9 37 10  3]\n",
            " [10 17 39  3]\n",
            " [ 5  5  1 32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Afficher la matrice de confusion\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "K4DCWSlWINJZ",
        "outputId": "9d075651-07da-402d-a8e4-ae9689a2d73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}